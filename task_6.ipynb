{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euigdTiKg7DR"
   },
   "source": [
    "# Машинное обучение, ВМК МГУ\n",
    "\n",
    "# Практическое задание 2. Обучение без учителя.\n",
    "\n",
    "## Общая информация\n",
    "Дата выдачи: 19.03.2023\n",
    "\n",
    "Мягкий дедлайн: 03.04.2021 23:59 MSK **(за каждый день просрочки снимается 1 балл)**\n",
    "\n",
    "Жёсткий дедлайн: 10.04.2021 23:59 MSK\n",
    "\n",
    "## Оценивание и штрафы\n",
    "\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — **10 баллов + 4.4 бонусов**.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "## Формат сдачи\n",
    "Задания сдаются через систему anytask. Посылка должна содержать:\n",
    "* Ноутбук homework-practice-02-Username.ipynb\n",
    "\n",
    "Username — ваша фамилия на латинице"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgNexc1dnvV-"
   },
   "source": [
    "<p style=\"color:#de3815;font-size:25px;\">\n",
    "Напоминание об оформлении и выполнении ноутбука\n",
    "</p>\n",
    "\n",
    "* Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть, не запуская ячейки (кроме редких случаев, когда необходимо намеренно скрыть ненужный output, про такие случаи желательно писать пояснения в тексте). **В противном случае -1 балл**\n",
    "* При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст, если явно не указана такая возможность. **В противном случае -1 балл**\n",
    "* В anytask обязательно нужно прикреплять отдельно файл с расширением ipynb (не в архиве, а именно отдельно). Если необходимо отправить еще какие-то файлы, то вынесите их в отдельный архив (если файлов много) и пришлите. **В противном случае -0.5 балла**\n",
    "---\n",
    "* Пишите, пожалуйста, выводы и ответы на вопросы в текстовых ячейках/при помощи print в коде. При их отсутствии мы не можем понять, сделали ли вы задание и понимаете, что происходит, и **поэтому будем снижать баллы**\n",
    "* Если алгоритм не сказано реализовывать явно, его всегда можно импортировать из библиотеки.\n",
    "---\n",
    "* Про графики. _Штрафы будут применяться к каждому результату команды отображения графика (plt.show() и др. аналогичные). Исключением являются графики, генерируемые функциями каких-либо сторонних библиотек, если их нельзя кастомизировать_\n",
    "\n",
    "    * должно быть название (plt.title) графика; **В противном случае &ndash; -0.05 балла**\n",
    "    * на графиках должны быть подписаны оси (plt.xlabel, plt.ylabel); **В противном случае &ndash; -0.025 балла за каждую ось**\n",
    "    * должны быть подписаны единицы измерения (если это возможно); **В противном случае &ndash; -0.025 балла за каждую ось**\n",
    "    * все названия должны быть понятны любому человеку, знакомому с терминологией, без заглядывания в код; **В противном случае &ndash; -0.05 балла**\n",
    "    * подписи тиков на осях не должны сливаться как на одной оси, так и между ними; **В противном случае &ndash; -0.025 балла за каждую ось**\n",
    "    * если изображено несколько сущностей на одном холсте (например несколько функций), то необходима поясняющая легенда (plt.legend); **В противном случае &ndash; -0.05 балла**\n",
    "    * все линии на графиках должны быть чётко видны (нет похожих цветов или цветов, сливающихся с фоном); **В противном случае &ndash; -0.05 балла**\n",
    "    * если отображена величина, имеющая очевидный диапазон значений (например, проценты могут быть от 0 до 100), то желательно масштабировать ось на весь диапазон значений (исключением является случай, когда вам необходимо показать малое отличие, которое незаметно в таких масштабах);\n",
    "    * графики должны быть не супер-микро и не супер-макро по размерам, так, чтобы можно было увидеть все, что нужно.\n",
    "    * при необходимости улучшения наглядности графиков, можно пользоваться логарифмической шкалой по осям x/y.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34CkHzFqnvV_"
   },
   "source": [
    "### А также..\n",
    "\n",
    "* Для удобства поиска вопросов, на которые от вас просят ответа, мы пометили их знаком **(?)**\n",
    "* Знак **(!)** означает, что выполнение замечания необходимо для **возможности получения полного балла**\n",
    "* Даем до +0.3 баллов за выдающиеся успехи по субъективному мнению проверяющих. Этот **бонус** не апеллируется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "068oO9TsnvWA"
   },
   "source": [
    "## О задании\n",
    "\n",
    "В этом задании мы посмотрим на несколько алгоритмов кластеризации и применим их к географическим и текстовым данным. Также мы подробно остановимся на тематическом моделировании текстов, задаче обучения представлений и в каком-то смысле поработаем с semi-supervised learning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8oZVUmTnvWA"
   },
   "source": [
    "**Замечание:**\n",
    "* seed менять нежелательно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95cdXa4PX8ks"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0xFFFFFFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK5GdpMuppA3"
   },
   "source": [
    "## Часть 1. Кластеризация автобусных остановок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVdORZxZiZTS"
   },
   "source": [
    "В этом задании мы сравним разные алгоритмы кластеризации для данных об автобусных остановках Москвы.\n",
    "\n",
    "**Задание 1.1 (1 балл).** Реализуйте алгоритм спектральной кластеризации, который упоминался на лекции/семинаре. Для этого разберитесь с кодом шаблона, данного ниже, и допишите недостающую функцию. Напомним, что для графа с матрицей смежности $W = \\{w_{ij}\\}_{i, j = 1 \\dots \\ell}$ лапласиан определяется как:\n",
    "\n",
    "$$\n",
    "L = D - W,\n",
    "$$\n",
    "\n",
    "где $D = \\text{diag}(d_1, ..., d_{\\ell}), d_i = \\sum_{j=1}^{\\ell} w_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uVuBJPBixGT"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import ClusterMixin\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "class GraphClustering(ClusterMixin):\n",
    "    def __init__(self, n_clusters=8, n_components=None, **kwargs):\n",
    "        '''\n",
    "        Spectral clustering algorithm\n",
    "        param n_clusters: number of clusters to form\n",
    "        param n_components: number of eigenvectors to use\n",
    "        '''\n",
    "\n",
    "        if n_components is None:\n",
    "            n_components = n_clusters\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, **kwargs)\n",
    "\n",
    "    def fit_predict(self, X, y=None):\n",
    "        '''\n",
    "        Perform spectral clustering from graph adjacency matrix\n",
    "        and return vertex labels.\n",
    "        param X: (n_samples, n_samples) - graph adjacency matrix\n",
    "        return: (n_samples, ) - vertex labels\n",
    "        '''\n",
    "\n",
    "        eigenvectors = self._generate_eigenvectors(X)\n",
    "        labels = self.kmeans.fit_predict(eigenvectors[:, 1:])\n",
    "        return labels\n",
    "\n",
    "    def _generate_eigenvectors(self, X):\n",
    "        '''\n",
    "        Compute eigenvectors for spectral clustering\n",
    "        param X: (n_samples, n_samples) - graph adjacency matrix\n",
    "        return: (n_samples, n_components) - eigenvectors\n",
    "        '''\n",
    "        D = np.diag(np.sum(X, axis=1))\n",
    "        L = D - X\n",
    "        w, v = np.linalg.eigh(L)\n",
    "        return v[:, :self.n_components]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOFgh62FoKPB"
   },
   "source": [
    "Перед тем, как переходить к следующему заданию, протестируйте свое решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvMVPvHYoPw_"
   },
   "outputs": [],
   "source": [
    "n_blocks, n_vertices = 10, 1000\n",
    "block_vertices = n_vertices // n_blocks\n",
    "\n",
    "X = np.zeros((n_vertices, n_vertices))\n",
    "for i in range(0, n_vertices, block_vertices):\n",
    "    X[i:i + block_vertices, i:i + block_vertices] = np.sqrt(i + 1)\n",
    "\n",
    "graph_clustering = GraphClustering(n_clusters=n_blocks)\n",
    "labels = graph_clustering.fit_predict(X)\n",
    "\n",
    "true_labels = np.zeros(n_vertices, dtype=np.int32)\n",
    "for i in range(0, n_vertices, block_vertices):\n",
    "    true_labels[i:i + block_vertices] = labels[i]\n",
    "\n",
    "assert labels.shape == (n_vertices, )\n",
    "assert np.all(np.bincount(labels) == np.full(n_blocks, block_vertices))\n",
    "assert np.all(labels == true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhGkjibXnvWH"
   },
   "source": [
    "**Замечание**\n",
    "* Для возможности получения **полного балла** assert должны проходить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ueXlg87of70"
   },
   "source": [
    "Теперь можем приступить к работе с реальными данными. Скачайте файл с данными об остановках общественного транспорта **в формате .xlsx** по [ссылке](https://data.mos.ru/opendata/download/60464/1/201) (так гарантированно не возникнет проблем с парсингом файла) и загрузите таблицу в ноутбук. Если вдруг сайт Правительства Москвы сойдет с ума, то возьмите какую-нибудь версию данных [отсюда](https://data.mos.ru/opendata/7704786030-city-surface-public-transport-stops). Для удобства визуализации мы будем работать только с остановками в ЦАО."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwTVjBaZnvWH"
   },
   "source": [
    "**Замечание**\n",
    "* (?) Напишите, пожалуйста, откуда в итоге скачивали данные. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPMxIElnuNcR"
   },
   "source": [
    "Данные скачал по первой ссылке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "6WWxngQO8Jt3",
    "outputId": "68be26d6-d46b-4772-bb69-155d172cd3ba"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('transport.xlsx')\n",
    "data = data[data.AdmArea_en == \"Czentral`ny'j administrativny'j okrug\"]\n",
    "data = data.reset_index()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEJdUfnY9qFj"
   },
   "source": [
    "Воспользуемся библиотекой `folium` для визуализации данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "KosUV23W9xgn",
    "outputId": "157b0f1d-1b48-466f-a6ef-e9d5e07f993b"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "map_ = folium.Map([55.75215, 37.61819], zoom_start=12)\n",
    "for id, row in data.iterrows():\n",
    "    folium.Circle([row.Latitude_WGS84_en, row.Longitude_WGS84_en],\n",
    "                  radius=10).add_to(map_)\n",
    "map_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4oB0OQk-MLP"
   },
   "source": [
    "**Задание 1.2 (1 балл).** Попробуем построить граф, в котором вершинами будут остановки. Как вы уже могли заметить, для каждой остановки указаны номера маршрутов, проходящих через неё. Логично соединить ребрами соседние остановки каждого маршрута. Однако мы не знаем, в каком порядке автобусы объезжают остановки. Но мы можем применить эвристический алгоритм, который восстановит нам порядок маршрутов:\n",
    "\n",
    "* Для каждого маршрута выделим список всех остановок, через которые он проходит.\n",
    "* Выберем начальную остановку маршрута как точку, наиболее удаленную от всех остальных остановок этого маршрута.\n",
    "* Каждую следующую точку маршрута будем выбирать из оставшихся (не включенных еще в маршрут) точек, как самую близкую к последней найденной точке (последней, включенной в маршрут)\n",
    "\n",
    "Фактически, у нас получается жадное решение задачи коммивояжера. Когда мы отсортировали маршруты, можем построить по ним граф. Будем строить его по таким правилам:\n",
    "\n",
    "* Между двумя остановками будет ребро, если они являются соседними хотя бы на одном маршруте. Вес ребра равен числу маршрутов, на которых остановки являются соседними.\n",
    "* В графе не будет петель (то есть у матрицы смежности будет нулевая диагональ).\n",
    "\n",
    "Реализуйте предложенный способ построения графа. Для этого рекомендуется воспользоваться шаблонами, приведенными ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJTwEzjitM0c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from collections import defaultdict\n",
    "# from haversine import haversine\n",
    "\n",
    "def get_routes(data):\n",
    "    '''\n",
    "    Accumulate routes from raw data\n",
    "    param data: pd.DataFrame - public transport stops data\n",
    "    return: dict - unsorted stops ids for each route,\n",
    "                   e.g. routes['A1'] = [356, 641, 190]\n",
    "    '''\n",
    "    RouteNumbers = data['RouteNumbers_en']\n",
    "    res = defaultdict(list)\n",
    "    for stop, routes in RouteNumbers.items():\n",
    "      routes_list = [x.strip() for x in str(routes).split(';')]\n",
    "      for route in routes_list:\n",
    "        res[route].append(stop)\n",
    "    return dict(res)\n",
    "\n",
    "def sort_routes(data, routes):\n",
    "    '''\n",
    "    Sort routes according to the proposed algorithm\n",
    "    param data: pd.DataFrame - public transport stops data\n",
    "    param routes: dict - unsorted stops ids for each route\n",
    "    return: dict - sorted stops ids for each route\n",
    "    '''\n",
    "    res = dict()\n",
    "    for route, stops in routes.items():\n",
    "      coordinates = data.loc[stops, ['Latitude_WGS84_en', 'Longitude_WGS84_en']]\n",
    "      distances = haversine_distances(np.radians(coordinates))\n",
    "      first_stop = np.argmax(np.sum(distances, axis=1))\n",
    "      new_stops = [coordinates.index.values[first_stop]]\n",
    "      distances[:, first_stop] = 999999999\n",
    "      next_stop = first_stop\n",
    "      while len(stops) != len(new_stops):\n",
    "        next_stop = np.argmin(distances[next_stop])\n",
    "        distances[:, next_stop] = 999999999\n",
    "        new_stops.append(coordinates.index.values[next_stop])\n",
    "        distances[:, next_stop] = 999999999\n",
    "      res[route] = new_stops\n",
    "    return res\n",
    "\n",
    "def get_adjacency_matrix(data, sorted_routes):\n",
    "    '''\n",
    "    Compute adjacency matrix for sorted routes\n",
    "    param data: pd.DataFrame - public transport stops data\n",
    "    param sorted_routes: dict - sorted stops ids for each route\n",
    "    return: (n_samples, n_samples) - graph adjacency matrix\n",
    "    '''\n",
    "    adjacency_matrix = np.zeros((len(data), len(data)))\n",
    "    for stops in sorted_routes.values():\n",
    "      adjacency_matrix[stops[:-1], stops[1:]] += 1\n",
    "      adjacency_matrix[stops[1:], stops[:-1]] += 1\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2fMkb9VwBIr"
   },
   "outputs": [],
   "source": [
    "routes = get_routes(data)\n",
    "sorted_routes = sort_routes(data, routes)\n",
    "adjacency_matrix = get_adjacency_matrix(data, sorted_routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot-EUmMKwEx1"
   },
   "source": [
    "Проверим, что маршруты получились адекватными. Для этого нарисуем их на карте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "P1d67q5oxPHs",
    "outputId": "6df4b4d0-e233-4ec9-8355-88c73af7c5cf"
   },
   "outputs": [],
   "source": [
    "map_ = folium.Map([55.75215, 37.61819], zoom_start=12)\n",
    "for route_id in np.random.choice(list(sorted_routes.keys()), size=5):\n",
    "    coords = data.loc[\n",
    "        sorted_routes[route_id],\n",
    "        ['Latitude_WGS84_en', 'Longitude_WGS84_en']\n",
    "    ].values.tolist()\n",
    "    folium.vector_layers.PolyLine(coords).add_to(map_)\n",
    "\n",
    "map_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c03XzwlS_bmU"
   },
   "source": [
    "**Задание 1.3 (0 баллов)**. Реализуйте функцию `draw_clustered_map`, которая рисует карту центра Москвы с кластерами остановок, раскрашенными в разные цвета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQVv_bQI34TG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import to_hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwA4RxiC65Xn"
   },
   "outputs": [],
   "source": [
    "def draw_clustered_map(data, labels):\n",
    "    '''\n",
    "    Create map with coloured clusters\n",
    "    param data: pd.DataFrame - public transport stops data\n",
    "    param labels: (n_samples, ) - cluster labels for each stop\n",
    "    return: folium.Map - map with coloured clusters\n",
    "    '''\n",
    "    map_ = folium.Map([55.75215, 37.61819], zoom_start=12)\n",
    "    colors = []\n",
    "    colormap = cm.get_cmap('CMRmap')\n",
    "    for value in np.linspace(0, 1, len(np.unique(labels))):\n",
    "      color = colormap(value)\n",
    "      colors.append(to_hex(color))\n",
    "    for latitude, longitude, label in zip(data['Latitude_WGS84_en'], data['Longitude_WGS84_en'], labels):\n",
    "      folium.Circle([latitude, longitude], radius=100, color=colors[label], tooltip=\"Класс \" + str(label)).add_to(map_)\n",
    "    return map_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYnX3Ws__ga0"
   },
   "source": [
    "**Задание 1.4 (1.5 балла)**. \n",
    "\n",
    "* **(0.25 балла)** Примените алгоритмы кластеризации K-Means и DBSCAN из `sklearn` на координатах остановок, а также свою реализацию спектральной кластеризации на построенной выше матрице смежности. В этом пункте используйте дефолтные параметры.\n",
    "* **(0.25 балла)** Визуализируйте результат кластеризации из предыдущего пункта с помощью функции `draw_clustered_map`.\n",
    "* **(0.75 балла)** А теперь подберите с помощью **кросс-валидации** параметры алгоритмов (`n_clusters` у K-Means, `eps` у DBSCAN, `n_clusters` и `n_components` у спектральной кластеризации) так, чтобы получить наиболее характерный для этих алгоритмов результат кластеризации. В качестве метрики качества можете использовать `silhouette_score`. Визуализируйте полученные результаты для всех трех алгоритмов на *оптимальных параметрах*\n",
    "* (?) Проинтерпретируйте полученные результаты.\n",
    " * **(0.1 балла)** Чем отличаются разбиения на кластеры, получаемые разными алгоритмами?\n",
    " * **(0.1 балла)** Какие плюсы и минусы есть у каждого алгоритма?\n",
    " * **(0.05 балла)** Какой алгоритм кажется вам наиболее подходящим для кластеризации остановок?\n",
    "\n",
    "**Подсказка:**\n",
    "* Не забудьте, что DBSCAN помечает некоторые точки как шумовые (можно раскрасить их в отдельный цвет).\n",
    "* Обращайте внимание на баланс объектов в кластерах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RliT89AL8vA9"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_KvTSJCuQB2"
   },
   "source": [
    "### Применение и визуализация всех трех алгоритмов кластеризации на дефолтных параметрах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRvwu-vIuwcx"
   },
   "outputs": [],
   "source": [
    "X = data[['Longitude_WGS84_en', 'Latitude_WGS84_en']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "TZzf8wtUrSjH",
    "outputId": "c1cd5ba8-a6f3-4c33-ce87-1f6f39ebdf7a"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans().fit(X)\n",
    "y = kmeans.predict(X)\n",
    "draw_clustered_map(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "egGiYrjLuqYU",
    "outputId": "e3a0dc93-ba8a-4b6c-de9d-a06212e6587c"
   },
   "outputs": [],
   "source": [
    "y = DBSCAN().fit_predict(X)\n",
    "draw_clustered_map(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "G-9IcayWvJO2",
    "outputId": "b2edca1c-eff1-4c3a-df4e-469169648694"
   },
   "outputs": [],
   "source": [
    "y = GraphClustering().fit_predict(adjacency_matrix)\n",
    "draw_clustered_map(data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIWvU2HmuKDH"
   },
   "source": [
    "### Подбор параметра n_clusters для KMeans (по коэффициенту силуэта)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gF6kdXLxLaCE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOVZJ-4b6xYC"
   },
   "outputs": [],
   "source": [
    "n_clusters_list = np.linspace(2, 500, num=50, dtype=int)\n",
    "silhouettes_list = []\n",
    "for n_clusters in n_clusters_list:\n",
    "  kmeans = KMeans(n_clusters=n_clusters).fit(X)\n",
    "  y = kmeans.predict(X)\n",
    "  silhouettes_list.append(silhouette_score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9fDxm7DLYVv"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "AluMpG_rnJmE",
    "outputId": "a2ea8f0d-1f4d-4cfe-bc2f-7605eed19e27"
   },
   "outputs": [],
   "source": [
    "plt.title('Зависимость коэффициента силуэта от n_clusters для KMeans')\n",
    "max_silhouette_num = np.argmax(silhouettes_list)\n",
    "optim_n_clusters = n_clusters_list[max_silhouette_num]\n",
    "plt.plot(n_clusters_list, silhouettes_list)\n",
    "plt.xlabel('n_clusters')\n",
    "plt.ylabel('коэффициент силуэта')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aciG4-iroZ-R",
    "outputId": "5ff3f19e-8c28-494d-e7ec-43d1bbda05a8"
   },
   "outputs": [],
   "source": [
    "print('Оптимальное значение n_clusters = ', optim_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "IM9iPnJjokdD",
    "outputId": "53b1c963-1e1f-41d6-973c-afc592eb2011"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optim_n_clusters).fit(X)\n",
    "y = kmeans.predict(X)\n",
    "draw_clustered_map(data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVvmbFg8shVm"
   },
   "source": [
    "Получилось что-то непонятное. Сузим диапазон для n_clusters от [2,500] до [3,100]. 2 выкидывается, так как нас такой вариант явно не интересует."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiw0uyvfsuss"
   },
   "outputs": [],
   "source": [
    "n_clusters_list = np.linspace(3, 100, num=50, dtype=int)\n",
    "silhouettes_list = []\n",
    "for n_clusters in n_clusters_list:\n",
    "  kmeans = KMeans(n_clusters=n_clusters).fit(X)\n",
    "  y = kmeans.predict(X)\n",
    "  silhouettes_list.append(silhouette_score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "yraFEuZ4s_jD",
    "outputId": "5f54af53-f2cf-44a4-cc7d-6630d6659319"
   },
   "outputs": [],
   "source": [
    "plt.title('Зависимость коэффициента силуэта от n_clusters для KMeans')\n",
    "plt.plot(n_clusters_list, silhouettes_list)\n",
    "max_silhouette_num = np.argmax(silhouettes_list)\n",
    "optim_n_clusters = n_clusters_list[max_silhouette_num]\n",
    "plt.xlabel('n_clusters')\n",
    "plt.ylabel('коэффициент силуэта')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2imhhTA9tMpd",
    "outputId": "efe67a68-efa4-4847-e107-7afc49b093f3"
   },
   "outputs": [],
   "source": [
    "print('Оптимальное значение n_clusters = ', optim_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "L_vJY2oItPEq",
    "outputId": "eacd1af2-7497-449d-f4ce-48cae6d71fbb"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optim_n_clusters).fit(X)\n",
    "y = kmeans.predict(X)\n",
    "draw_clustered_map(data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWKh2EF9tq_J"
   },
   "source": [
    "Кажется, что такой результат лучше, чем предыдущий. Кластеры в какой-то степени напоминают маршруты, однако есть те, у которых малое количество объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T2CPT23um23"
   },
   "source": [
    "### Подбор параметра eps для DBSCAN (визуально)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN0C_9RP2Aub"
   },
   "source": [
    "Выше, применяя DBSCAN для параметра по умолчанию (eps=0.5) все объекты были отнесены к одному кластеру. Попробуем найти диапазон, нижний предел которого соответствует кластеризации, в которой есть хотя бы не один шумовой объект, а верхний предел которого соответствует кластеризации с более, чем двумя кластерами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKQQsa9m50CJ"
   },
   "outputs": [],
   "source": [
    "eps_list = np.linspace(0.00001, 0.1, num=50)\n",
    "i = 0\n",
    "model = DBSCAN(eps=eps_list[0])\n",
    "y = model.fit_predict(X)\n",
    "while len(np.unique(y)) == 1:\n",
    "  i += 1\n",
    "  model = DBSCAN(eps=eps_list[i])\n",
    "  y = model.fit_predict(X)\n",
    "\n",
    "min_eps = eps_list[i]\n",
    "\n",
    "while not len(np.unique(y)) == 2:\n",
    "  i += 1\n",
    "  model = DBSCAN(eps=eps_list[i])\n",
    "  y = model.fit_predict(X)\n",
    "\n",
    "max_eps = eps_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGqQXHH36ub1",
    "outputId": "51227d01-7e5f-4372-8c51-cf99df2a2276"
   },
   "outputs": [],
   "source": [
    "print('Нижний предел = ', min_eps)\n",
    "print('Верхний предел = ', max_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1sKCKr267ul"
   },
   "source": [
    "Основываясь на этом отрезке, подберем отпимальное значение eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "id": "WcW2dkBkKPEc",
    "outputId": "e0b2cbaa-0f6f-4263-e888-f19a35657bc1"
   },
   "outputs": [],
   "source": [
    "eps_list = np.linspace(min_eps, max_eps, num=20)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.suptitle('DBSCAN кластеризация при разных eps (шумовые объекты отмечены серым)')\n",
    "# colormap = cm.get_cmap('CMRmap')\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        model = DBSCAN(eps=eps_list[i*5+j])\n",
    "        y = model.fit_predict(X)\n",
    "        plt.subplot(4, 5, 1+i*5+j)\n",
    "        plt.scatter(X.iloc[y != -1, 0], X.iloc[y != -1, 1], c=y[y!=-1], cmap='CMRmap', s=2)\n",
    "        plt.scatter(X.iloc[y == -1, 0], X.iloc[y == -1, 1], color='gray', s=2)\n",
    "        plt.axis('off')\n",
    "        plt.title('eps = ' + f'{eps_list[i*5+j]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FejFYYNoOOQw"
   },
   "source": [
    "Наиболее подходящим кажется eps=0.0036.\n",
    "Выведем теперь результат на карте (у шумовых объектов класс = -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "IpYKiUxZ-Suc",
    "outputId": "06f64865-0aa7-4cab-9ef5-ff943563c4b7"
   },
   "outputs": [],
   "source": [
    "y = DBSCAN(eps=0.0036).fit_predict(X)\n",
    "draw_clustered_map(data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfM22LV8_WSx"
   },
   "source": [
    "### Подбор параметров n_clusters и n_components для спектральной кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFJCbMdO_ju-"
   },
   "source": [
    "Для начала подберем оптимальное количество кластеров, а затем количество собственных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "id": "xxU8CWxIG5Fx",
    "outputId": "d2d053ec-e98b-4a05-b10c-9f43721e1d8f"
   },
   "outputs": [],
   "source": [
    "n_clusters_list = np.linspace(3, 100, num=20, dtype=int)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.suptitle('Спектральная кластеризация при разных n_clusters')\n",
    "# colormap = cm.get_cmap('CMRmap')\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        y = GraphClustering(n_clusters=n_clusters_list[i*5+j]).fit_predict(adjacency_matrix)\n",
    "        plt.subplot(4, 5, 1+i*5+j)\n",
    "        plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap='CMRmap', s=2)\n",
    "        plt.axis('off')\n",
    "        plt.title('n_clusters = ' + f'{n_clusters_list[i*5+j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yvr-sjXbQASf"
   },
   "source": [
    "Рассмотрим внимательнее значения от 38 до 48."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "e2qtXJvuQFNI",
    "outputId": "1e10018a-23d4-4f3b-db20-0677f0e57783"
   },
   "outputs": [],
   "source": [
    "n_clusters_list = [x for x in range(38,49)]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Спектральная кластеризация при разных n_clusters')\n",
    "# colormap = cm.get_cmap('CMRmap')\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        y = GraphClustering(n_clusters=n_clusters_list[i*5+j]).fit_predict(adjacency_matrix)\n",
    "        plt.subplot(2, 5, 1+i*5+j)\n",
    "        plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap='gnuplot', s=2)\n",
    "        plt.axis('off')\n",
    "        plt.title('n_clusters = ' + f'{n_clusters_list[i*5+j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8yheknJRDYl"
   },
   "source": [
    "При 41 картинка выглядит вполне реалистичной.\n",
    "Теперь подберем количество собственных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "dSDGDKdxRMK6",
    "outputId": "9622e6c3-b8b8-4ade-a2ed-c5c1956dcdbe"
   },
   "outputs": [],
   "source": [
    "n_components_list = np.linspace(3, 100, num=20, dtype=int)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.suptitle('Спектральная кластеризация при разных n_components (n_clusters=41)')\n",
    "# colormap = cm.get_cmap('CMRmap')\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        y = GraphClustering(n_clusters=41, n_components=n_components_list[i*5+j]).fit_predict(adjacency_matrix)\n",
    "        plt.subplot(4, 5, 1+i*5+j)\n",
    "        plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap='CMRmap', s=2)\n",
    "        plt.axis('off')\n",
    "        plt.title('n_components = ' + f'{n_components_list[i*5+j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TpzJihGR4xv"
   },
   "source": [
    "Остановимся на 43 собственных векторах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "qR6Gt_uMLVmd",
    "outputId": "649ce7b0-d7b0-465f-bff2-26933b27ddcf"
   },
   "outputs": [],
   "source": [
    "y = GraphClustering(n_clusters=41, n_components=43).fit_predict(adjacency_matrix)\n",
    "draw_clustered_map(data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNYIPsGHNtuY"
   },
   "source": [
    "Здесь уже действительно можно разглядеть маршруты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIZIvcM8BF_I"
   },
   "source": [
    "**Ваш ответ здесь:**\n",
    "\n",
    "*1) Чем отличаются разбиения на кластеры, получаемые разными алгоритмами?*\n",
    "\n",
    "В KMeans кластеры представляют собой некоторые маленькие участки, отчасти напоминающие маршруты, но не совсем они. В DBSCAN получаются более сложные участки, но есть кластеры в которых очень мало объектов, а есть в которых очень много. В спектральной кластеризации получилось увидеть маршруты, некоторые из которых даже пересекаются, но некоторые дефекты (в виде кластеров из малого количества объектов) остались, хотя таких кластеров не очень много.\n",
    "\n",
    "*2) Какие плюсы и минусы есть у каждого алгоритма?*\n",
    "\n",
    "Плюс спектральной кластеризации - кластеры лучше всего напоминают маршруты, причем маршруты пересекаются, минус - вычислительно более затратно.\n",
    "\n",
    "Плюс KMeans - быстрота, минус - кластеры не подходят для определения маршрутов.\n",
    "\n",
    "Плюс DBSCAN - чуть лучше KMeans в плане распознавания маршрутов, но есть дефекты со слишком большими и слишком малыми кластерами.\n",
    "\n",
    "*3) Какой алгоритм кажется вам наиболее подходящим для кластеризации остановок?*\n",
    "\n",
    "Спектральная кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr3WXyPBptaN"
   },
   "source": [
    "## Часть 2. Тематическое моделирование текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lh2yJqxhigf4"
   },
   "source": [
    "В этой части мы познакомимся с одной из самых популярных задач обучения без учителя &mdash; с задачей тематического моделирования текстов. Допустим, нам доступна некоторая коллекция документов без разметки, и мы хотим автоматически выделить несколько тем, которые встречаются в документах, а также присвоить каждому документу одну (или несколько) тем. Фактически, мы будем решать задачу, похожую на кластеризацию текстов: отличие в том, что нас будет интересовать не только разбиение текстов на группы, но и выделение ключевых слов, определяющих каждую тему.\n",
    "\n",
    "Мы будем работать с новостными статьями BBC за 2004-2005 годы. Скачайте данные по [ссылке](https://www.kaggle.com/hgultekin/bbcnewsarchive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UMefb5XsixgH",
    "outputId": "8e703885-81a4-4982-ddc6-5e6c63f1757e"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('bbc-news-data.csv', sep='\\t')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMiigCjtY0yh"
   },
   "source": [
    "Как вы могли заметить, данные уже содержат разметку по тематике (колонка category). В этой части мы забудем, что она есть🙈, и будем работать только с текстовыми данными. Проведем предобработку текста, состоящую из следующих пунктов:\n",
    "\n",
    "* Объединим заголовок и содержание статьи в одно поле.\n",
    "* Приведем текст к нижнему регистру, разобьем его на токены.\n",
    "* Оставим только буквенные слова (удалив, таким образом, пунктуацию и числа).\n",
    "* Применим лемматизацию.\n",
    "* Удалим стоп-слова.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpSY0M7kbIdl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk3yw5aNbRgi",
    "outputId": "afbb3e86-7d79-4cc4-a0bf-028f4cf1bc00"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english') + ['ha', 'wa', 'say', 'said'])\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbMsHeS2bV2l"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = list(filter(str.isalpha, word_tokenize(text.lower())))\n",
    "    text = list(lemmatizer.lemmatize(word) for word in text)\n",
    "    text = list(word for word in text if word not in stop_words)\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhICemtQbg0o"
   },
   "outputs": [],
   "source": [
    "data['raw_text'] = data.apply(lambda row: row.title + row.content, axis=1)\n",
    "data['text'] = data.apply(lambda row: preprocess(row.raw_text), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0O5ygZPcTkl"
   },
   "source": [
    "Для визуализации частот слов в текстах мы будем использовать [облака тегов](https://en.wikipedia.org/wiki/Tag_cloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRo7Q2bXczEZ"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def draw_wordcloud(texts, max_words=1000, width=1000, height=500):\n",
    "    wordcloud = WordCloud(background_color='white', max_words=max_words,\n",
    "                          width=width, height=height)\n",
    "    \n",
    "    joint_texts = ' '.join(list(texts))\n",
    "    wordcloud.generate(joint_texts)\n",
    "    return wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "bTnO8HwGdSvA",
    "outputId": "7b96b81e-87df-4253-bbb7-8e5ef0880818"
   },
   "outputs": [],
   "source": [
    "draw_wordcloud(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UQN3YL3nvWN"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MoyLGx-dcjF"
   },
   "source": [
    "**Задание 2.1 (1 балл).** \n",
    "\n",
    "* **(0.5 балла)** Обучите алгоритм K-Means на tf-idf представлениях текстов. \n",
    "* **(0.25 балла)** Постройте облака тегов для текстов из разных кластеров.\n",
    "* **(0.25 балла)** (?) Получились ли темы интерпретируемыми? (?) Попробуйте озаглавить каждую тему.\n",
    "\n",
    "**Подсказки:**\n",
    "* Можно использовать готовую реализацию tf-idf векторайзера из sklearn\n",
    "* При обучении tf-idf векторайзера рекомендуется отбрасывать редко встречающиеся слова.\n",
    "* Воздержитесь от использования N-грамм при обучении tf-idf.\n",
    "* Возьмите не очень большое число кластеров, чтобы было удобно интерпретировать получившиеся темы (например, `n_clusters` = 8).\n",
    "\n",
    "**Ключевые слова:**\n",
    "* *TfidfVectorizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cr65aXYRU7f2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 1e-5)\n",
    "X = vectorizer.fit_transform(data.text)\n",
    "kmeans = KMeans(n_clusters=9, random_state=42) \n",
    "themes = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iM8YVPiiGN7B"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "JnBFM4vsejH9",
    "outputId": "663cbe1d-be27-4bca-b193-da08c625d449"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(9):\n",
    "  plt.subplot(3, 3, i+1)\n",
    "  plt.axis('off')\n",
    "  image = draw_wordcloud(data.text[themes == i])\n",
    "  plt.imshow(image)\n",
    "plt.suptitle('Кластеризация KMeans')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yNdlm7GnvWN"
   },
   "source": [
    "**Ваш ответ здесь:**\n",
    "\n",
    "Было решение сделать 9 кластеров, так как при 8 некоторые темы пересекались.\n",
    "В целом темы получились интерпертируемыми.\n",
    "\n",
    "(i, j) означает картинку в i-й строке и j-м столбце\n",
    "\n",
    "1.   (1, 1) - компании, бизнес, правительство\n",
    "2.   (1, 2) - спорт, матчи\n",
    "3.   (1, 3) - компьютеры, программное обеспечение, пользователи\n",
    "4.   (2, 1) - экономика, рост, банки\n",
    "5.   (2, 2) - выборы, партии, труд\n",
    "6.   (2, 3) - кубок шести наций\n",
    "7.   (3, 1) - технологии, мобильные телефоны\n",
    "8.   (3, 2) - фильмы, оскар, актеры\n",
    "9.   (3, 3) - музыка, группы\n",
    "\n",
    "Можно заметить, что темы спорта и технологий встречались дважды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lhBcJzsnvWO"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b41ggvxRU_Oc"
   },
   "source": [
    "**Задание 2.2 (0.5 балла).** Попробуем другой способ выделить ключевые слова для каждой темы. Помимо непосредственного разбиения объектов алгоритм K-Means получает центр каждого кластера (`kmeans.cluster_centers_`). \n",
    "\n",
    "* **(0.3 балла)** Попробуйте взять центры кластеров и посмотреть на слова, для которых значения соответствующих им признаков tf-idf максимальны.\n",
    "* **(0.2 балла)** (?) Согласуются ли полученные слова с облаками тегов из прошлого задания?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYm-9i2bX7tJ",
    "outputId": "05a3463b-7adb-4a36-b21f-a8ade81c3eab"
   },
   "outputs": [],
   "source": [
    "features = np.array(vectorizer.get_feature_names_out())\n",
    "centers = np.argmax(kmeans.cluster_centers_, axis=1)\n",
    "print(features[centers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXf0BiphnvWO"
   },
   "source": [
    "**Ваш ответ здесь:**\n",
    "\n",
    "В целом, все слова согласуются с облаками тегов.\n",
    "\n",
    "mr - имеет наибольший вес в политике\n",
    "\n",
    "в теме кубка шести наций больший вес имеет england \n",
    "\n",
    "и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hG2Qen4nvWO"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjiogHYuYB0p"
   },
   "source": [
    "**Задание 2.3 (1.5 балла).** В первой части мы сравнили три разных алгоритма кластеризации на географических данных. Проделаем то же самое для текстовых данных (в качестве признакого описания снова используем tf-idf). \n",
    "\n",
    "* п.1 Получите три разбиения на кластеры с помощью алгоритмов K-Means, DBSCAN и спектральной кластеризации. \n",
    " * **(0.3 балла)** При этом для K-Means и спектральной кластеризации возьмите одинаковое небольшое число кластеров;\n",
    " * **(0.5 балла)** Подберите параметр `eps` метода DBSCAN так, чтобы получить приблизительно такое же число кластеров.\n",
    "* п.2 **(0.5 балла)** Обучите двухмерные t-SNE ([Почитать можно тут, часть 2](https://github.com/esokolov/ml-course-hse/blob/master/2020-fall/lecture-notes/lecture11-unsupervised.pdf), а также посмотреть непосредственно в [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)) представления над tf-idf признаками текстов. Визуализируйте эти представления для каждого алгоритма, раскрасив каждый кластер своим цветом.\n",
    "* п.3 **(0.2 балла)** (?) Прокомментируйте получившиеся результаты. (?) Какой баланс кластеров получился у разных методов? (?) Соотносятся ли визуализации для текстов с визуализациями для географических данных?\n",
    "\n",
    "\n",
    "**Замечания:**\n",
    "* В этом задании используйте реализации спектральной кластеризации и t-SNE из `sklearn`\n",
    "* В п.2 Лучше всего расположить визуализации на одном графике на трех разных сабплотах. \n",
    "\n",
    "**Подсказка:**\n",
    "* Не забудьте, что DBSCAN помечает некоторые точки как шумовые (можно раскрасить их в отдельный цвет)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwdrM6fe7jUi"
   },
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0fA0yEOYg_e"
   },
   "outputs": [],
   "source": [
    "kmeans_themes = themes # уже посчитали"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyv3XaTg7tDv"
   },
   "source": [
    "## Спектральная кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSPibubz7vTa"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "spectral_themes = SpectralClustering(n_clusters=9, random_state=42).fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9up6neg9Ro_"
   },
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98rsj32p9T-t"
   },
   "outputs": [],
   "source": [
    "eps_list = np.linspace(1, 1.5, num=100) # при меньших eps много шумовых объектов\n",
    "optim_eps = 0\n",
    "dbscan_themes = []\n",
    "for eps in eps_list:\n",
    "  themes = DBSCAN(eps=eps).fit_predict(X)\n",
    "  if ((len(np.unique(themes)) >= 8 and len(np.unique(themes)) <= 10)  and not (-1 in np.unique(themes)) or \n",
    "      (len(np.unique(themes)) >= 10 and len(np.unique(themes)) <= 11)):\n",
    "    optim_eps = eps\n",
    "    dbscan_themes = themes\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7vq1VLa_VUs",
    "outputId": "1f4bce2e-2742-438a-d38a-b50d52cca23a"
   },
   "outputs": [],
   "source": [
    "optim_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sST6eDGmC3ae",
    "outputId": "732d82ab-28be-41f5-8054-1e8b079b1c98"
   },
   "outputs": [],
   "source": [
    "len(dbscan_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-lfmxjgCpCw",
    "outputId": "1019d51d-3d78-426b-c52d-c4d8c0dd49fe"
   },
   "outputs": [],
   "source": [
    "len(dbscan_themes[dbscan_themes == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETyocopSAA6T",
    "outputId": "2586e444-fe25-4ba9-90b3-1ef3c55ef6b5"
   },
   "outputs": [],
   "source": [
    "np.unique(dbscan_themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OxmFDOoCT8q"
   },
   "source": [
    "Число кластеров для DBSCAN получилось равным 9. (еще есть один шумовой, который мы не относим к кластеру)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17BJcvnqCmbt"
   },
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvfVLx7aC7hX"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(random_state=42).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4ynKZnJEJaj"
   },
   "outputs": [],
   "source": [
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'lime', 'orange']\n",
    "names = ['KMeans', 'DBSCAN', 'SpectralClustering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwbOZrImFqQY"
   },
   "outputs": [],
   "source": [
    "t = [kmeans_themes, dbscan_themes, spectral_themes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "7whzfDLrEtS8",
    "outputId": "923cd842-cad1-448b-e6b0-14a3d80afc2d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "for i in enumerate(names):\n",
    "  plt.subplot(1, 3, i[0]+1)\n",
    "  plt.title(i[1])\n",
    "  if i[1] == 'DBSCAN':\n",
    "    plt.scatter(tsne[:, 0][t[i[0]] == -1], tsne[:, 1][t[i[0]] == -1], color='gray', s=2)\n",
    "  for j in range(9):\n",
    "    plt.scatter(tsne[:, 0][t[i[0]] == j], tsne[:, 1][t[i[0]] == j], color=colors[j], s=2)\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZo8daJrcyTR"
   },
   "source": [
    "**Ваш ответ здесь:**\n",
    "\n",
    "KMeans и спектральная кластеризация визуально показали более приемлемый результат, чем DBSCAN.\n",
    "\n",
    "У DBSCAN сильный дисбаланс классов (один большой кластер, один маленький, а остальные вовсе по несколько объектов). У KMeans наблюдается достаточно хороший баланс (есть один немного выделяющийся класс), а у спектральной кластеризации есть один кластер, который на вид достаточно больше других, но остальные кластеры в целом похожи (хоть есть один достаточно маленький).\n",
    "\n",
    "Для текстов в плане визуализации получились круглые скопления, а для маршрутов скорее некоторые кривые, отрезки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVO6dhmdnvWP"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7zArHv6dsVg"
   },
   "source": [
    "**Задание 2.4 (1.5 балла).** \n",
    "\n",
    "* **(0.5 балла)**  Обучите модель латентного размещения Дирихле (LDA).\n",
    "* **(0.5 балла)**  Придумайте как превратить распределение тем для текста в номер его кластера. Опишите текстом (?) и приведите код и результаты его работы\n",
    "* **(0.5 балла)** Возьмите параметр `n_components` у LDA в 2-3 раза больше, чем число кластеров для K-Means, и вновь посмотрите на темы. (?) Получились ли темы более узкими от такого нововведения? Постройте облака тегов для нескольких наиболее удачных тем.\n",
    "\n",
    "**Замечания:**\n",
    "* Реализацию LDA во всех заданиях используйте из `sklearn` (LatentDirichletAllocation)\n",
    "\n",
    "**Подсказка:**\n",
    "* Не забудьте, что LDA работает с мешком слов, а не с tf-idf признаками.\n",
    "\n",
    "**Ключевые слова:**\n",
    "* *CountVectorizer,  LatentDirichletAllocation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGw_tnc_dwgT"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "c_vectorizer = CountVectorizer(min_df=1e-5).fit_transform(data.text)\n",
    "lda = LatentDirichletAllocation(n_components=9, random_state=42)\n",
    "lda_themes = lda.fit_transform(c_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqOYgEziJoWZ"
   },
   "outputs": [],
   "source": [
    "lda_new = LatentDirichletAllocation(n_components=9*2, random_state=42)\n",
    "lda_themes_new = lda.fit_transform(c_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bLl86D4nvWP"
   },
   "source": [
    "**Ваш ответ здесь:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSPEXRQznvWP"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfWNOqFZii3J"
   },
   "source": [
    "## Часть 3. Transfer learning для задачи классификации текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CASUuV3jnvWP"
   },
   "source": [
    "**Замечание для всей части 3:**\n",
    "* Для возможности получения **полного балла** accuracy всех алгоритмов на тесте должны быть не ниже 0.75, а также хотя бы один из рассмотренных в этой части алгоритмов должен достигнуть точности не менее 0.95 на тесте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4clP98BXGLax"
   },
   "source": [
    "**Задание 3.1 (0.5 балла).** Вспомним, что у нас есть разметка для тематик статей 🌚🌝. Попробуем обучить классификатор поверх unsupervised-представлений для текстов. Рассмотрите три модели:\n",
    "\n",
    "* Логистическая регрессия на tf-idf признаках\n",
    "* K-Means на tf-idf признаках + логистическая регрессия на расстояниях до центров кластеров\n",
    "* Латентное размещение Дирихле + логистическая регрессия на вероятностях тем\n",
    "\n",
    "* **(0.4 балла)** Разделите выборку на обучающую и тестовую, замерьте accuracy на обоих выборках для всех трех моделей. Параметры всех моделей возьмите равными значениям по умолчанию.\n",
    "* **(0.1 балла)** (?) У какой модели получилось лучшее качество? (?) С чем это связано?\n",
    "\n",
    "**Подсказка:**\n",
    "* Во второй модели расстояния до центров кластеров можно получиться при помощи метода KMeans.transform\n",
    "\n",
    "**Ключевые слова:**\n",
    "* *OrdinalEncoder, TfidfVectorizer, CountVectorizer, train_test_split, KMeans.transform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7KiHqfkhO_R"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsbJHnR8gzHX"
   },
   "source": [
    "**Ваш ответ здесь:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEAZKm8wnvWQ"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18vaNs6XGScR"
   },
   "source": [
    "**Задание 3.2 (1.5 балла).** Теперь просимулируем ситуацию слабой разметки, которая часто встречается в реальных данных. Разделим обучающую выборку в пропорции 5:65:30. Будем называть части, соответственно, размеченный трейн, неразмеченный трейн и валидация.\n",
    "\n",
    "Все unsupervised-алгоритмы (векторайзеры (tf-idf, bag_of_words) и алгоритмы кластеризации (KMeans)) запускайте на всем трейне целиком (размеченном и неразмеченном, суммарно 70%), а итоговый классификатор обучайте только на размеченном трейне (5%).\n",
    "\n",
    "* **(1.2 балла)** Подберите гиперпараметры моделей по качеству на валидации (30%), а затем оцените качество на тестовой выборке (которая осталась от прошлого задания). Не скромничайте при подборе числа кластеров, сейчас нас интересует не интерпретируемое разбиение выборки, а итоговое качество классификации. \n",
    "* **(0.3 балла)** (?) Как изменились результаты по сравнению с обучением на полной разметке? (?) Сделайте выводы.\n",
    "\n",
    "**Пояснение:**\n",
    "* Тестовую выборку возьмите из задания 3.1. А разделять надо тренировочную из 3.1\n",
    "* Обучаете векторайзер на 70%, применяете векторайзер к 5%, и на этих 5% уже обучаете классификатор.\n",
    "* Итак, берем 3 варианта обучения из задания 3.1. Вы должны:\n",
    " * 1-я модель. Подобрать оптимальный гиперпараметр С у LogisticRegression на tf-idf признаках. Выбираем оптимальный параметр по валидационной выборке. Далее оцениваем обученную модель с оптимальном параметром на тесте.\n",
    " * 2-я модель. Шаг 1. Подобрать оптимальный гиперпараметр n_clusters у KMeans на tf-idf признаках: перебираем значения n_clusters и при каждом значении обучаем LogisticRegression на дефолтных параметрах (быть может с ограничением max_iter, об этом ниже). То есть классификатор в данном случае будет на одинаковых параметрах для всех представителей семейства алгоритмов KMeans. Далее выбираете оптимальный n_clusters по качеству классификатора на валидационной выборке. Шаг 2. Обучаете KMeans с оптимальным n_clusters и перебираете значение C у LogisticRegression. Также выбираете оптмальное по валидации. И только после двух подобранных оптимальных параметров выводите качество на тесте.\n",
    " * 3-я модель. Аналогично второй, только вместо KMeans --- LatentDirichletAllocation, а вместо tf-idf -- bag of words.\n",
    "\n",
    "**Замечания:**\n",
    "* Параметр $С$ у LogisticRegression перебирайте *по логарифмической сетке.* Обратите внимание, что $С$ НЕ перед регуляризатором, поэтому, скорее всего, наиболее оптимальными будут большие значения $C$. **Для возможности получения полного балла** переберите хотя бы 20 параметров, и затроньте значения в 1e-2 и 1e6 (по большей части значения должны лежать в этом отрезке)\n",
    "* **Для возможности получения полного балла** параметр n_clusters у KMeans при переборе должен затрагивать значения в 10 и 1000 кластеров (по большей части значения должны лежать в этом отрезке), а также должно быть перебрано не менее 20 значений кластеров *(по линейной сетке)*\n",
    "* **Для возможности получения полного балла** параметр n_components у LDA при переборе должен затрагивать значения в 5 и 14 (по большей части значения должны лежать в этом отрезке), а также должно быть перебрано не меньше 10 различных значений *(по линейной сетке)*\n",
    "\n",
    "**Подсказка:**\n",
    "* Может быть для ограничения времени обучения лучше будет ограничить параметр max_iter в LogisticRegression, равным, например, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n45ZJCDAGTQG"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMP7BslkkYAl"
   },
   "source": [
    "**Ваш ответ здесь:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6JuMul-nvWR"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTL9sgADKKah"
   },
   "source": [
    "## Бонус"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkAs4eBx8CXr"
   },
   "source": [
    "**Задание 4 (1 балл)**. \n",
    "\n",
    "* **(Максимум 0.8 балла, 0.4 балла за каждый исследованный метод)** Разберитесь с semi-supervised методами, которые реализованы в `sklearn` и примените их к заданию 3.2. \n",
    "* **(0.2 балла)** (?) Получилось ли добиться лучшего качества? (?) Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHTs3_ssKVG7"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOI6tcLoKVoS"
   },
   "source": [
    "**Задание 5 (1 балл)**. Существует метрика BCubed ([Ссылка 1](https://github.com/esokolov/ml-course-hse/blob/master/2020-spring/lecture-notes/lecture17-clusterization.pdf), [Ссылка 2](https://www.researchgate.net/profile/Julio-Gonzalo-2/publication/225548032_Amigo_E_Gonzalo_J_Artiles_J_et_alA_comparison_of_extrinsic_clustering_evaluation_metrics_based_on_formal_constraints_Inform_Retriev_12461-486/links/0c96052138dbb99740000000/Amigo-E-Gonzalo-J-Artiles-J-et-alA-comparison-of-extrinsic-clustering-evaluation-metrics-based-on-formal-constraints-Inform-Retriev-12461-486.pdf)), которая хорошо подходит для сравнения алгоритмов кластеризации, если нам известно настоящее разделение на кластеры (gold standard). \n",
    "\n",
    "* **(0.2 балла)** Реализуйте подсчет метрики BCubed\n",
    "* **(0.8 балла)** (?) Сравните несколько алгоритмов кластеризации на текстовых данных из основного задания. В качестве gold standard используйте разметку category.\n",
    "\n",
    "**Замечание:**\n",
    "* Пользоваться готовыми библиотечными методами для вычисления BCubed нельзя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQtZe7Ty847i"
   },
   "outputs": [],
   "source": [
    "def bcubed(L, C):\n",
    "  C_equal = L == L[:, np.newaxis]\n",
    "  L_equal = C == C[:, np.newaxis]\n",
    "  correctness = C_equal * L_equal\n",
    "  pr_bcubed = np.mean(np.mean(correctness, axis=1, where=C_equal))\n",
    "  r_bcubed = np.mean(np.mean(correctness, axis=1, where=L_equal))\n",
    "  return 2*pr_bcubed*r_bcubed/(pr_bcubed+r_bcubed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo_DH91LQiWv"
   },
   "outputs": [],
   "source": [
    "category = data['category']\n",
    "n_clusters = len(np.unique(category))\n",
    "category_numbers = pd.factorize(category)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDL_TxzsPw8c"
   },
   "outputs": [],
   "source": [
    "X = TfidfVectorizer(min_df=1e-5).fit_transform(data.text)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "spectral_clustering = SpectralClustering(n_clusters=n_clusters, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9j6khNDOQv1H",
    "outputId": "fd17fce8-df16-4dbf-a805-ae129bb68903"
   },
   "outputs": [],
   "source": [
    "print('KMeans. BCubed = ', bcubed(category_numbers, kmeans.fit_predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "od7wI7LWRsLg",
    "outputId": "45b9b8c3-4fe8-49c0-ce61-408883775e8e"
   },
   "outputs": [],
   "source": [
    "print('SpectralClustering. BCubed = ', bcubed(category_numbers, spectral_clustering.fit_predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ennqxomnvWR"
   },
   "source": [
    "**Ваш ответ здесь:**\n",
    "\n",
    "Результаты неплохие (а для KMeans даже хороший)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9oePmfZKc7s"
   },
   "source": [
    "**Задание 6 (2 баллa)**. Спектральная кластеризация, по сути, является обычной кластеризацией KMeans поверх эмбеддингов объектов, которые получаются из лапласиана графа. А что, если мы попробуем построить эмбеддинги каким-нибудь другим способом? В этом задании мы предлагаем вам проявить немного фантазии🧙.\n",
    "\n",
    "* Возьмите какие-нибудь данные высокой размерности, чтобы задача обучения эмбеддингов имела смысл (например, картинки или тексты, желательно выбрать что-нибудь оригинальное). (?) Напишите, откуда брали данные и что это за данные\n",
    "* **(1.7 балла)** Придумайте или найдите какой-нибудь метод обучения эмбеддингов, примените его к данным и кластеризуйте полученные представления. Если чувствуете в себе достаточно силы, можете попробовать что-нибудь нейросетевое.\n",
    "* **(0.3 балла)** (?) Сравните ваш подход с базовыми алгоритмами кластеризации, которое мы рассмотрели в основном задании, не забывайте про визуализации! \n",
    "\n",
    "**Подсказка:**\n",
    "* Ключевые слова для вдохновения: ***KernelPCA***, ***UMAP***, ***autoencoders***, ***gensim***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sb68ky4oKgkh"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pc2cw6ZA1KFW"
   },
   "source": [
    "**Задание 7 (0.1 балла)**. Опишите свои ощущения от этой домашки🥔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO2NKZVxS-cb"
   },
   "source": [
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQzxjT3PQhMZzYbE9i-Kkf5aAeAzvz8SoUd2Q&usqp=CAU\" style=\"width: 200px\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0_KvTSJCuQB2",
    "VIWvU2HmuKDH",
    "8T2CPT23um23",
    "bfM22LV8_WSx"
   ],
   "name": "homework-practice-02-YanakovDmitriy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
